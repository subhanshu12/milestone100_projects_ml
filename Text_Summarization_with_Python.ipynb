{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Text Summarization with Python***\n",
        "# Now, I will take you through the task of Text Summarization with Python. I will start by importing the necessary Python libraries:"
      ],
      "metadata": {
        "id": "-oAGLO6FZPKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xxmBXSpEZBSb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import string\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We don’t need to use a lot of machine learning here. We can easily summarize text without training a model. But still, we need to use some natural language processing, for that, I will be using the NLTK library in Python. \n",
        "\n",
        "# Now let’s perform some steps for removing punctuations from the text, then we need to do some steps of text processing, and at the end, we will simply tokenize the text and then you can see the results for text summarization with Python:"
      ],
      "metadata": {
        "id": "8dioyZXrZk-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Enter Text to Summarize\"\n",
        "if text.count(\". \") > 20:\n",
        "    length = int(round(text.count(\". \")/10, 0))\n",
        "else:\n",
        "    length = 1\n",
        "\n",
        "nopuch =[char for char in text if char not in string.punctuation]\n",
        "nopuch = \"\".join(nopuch)\n",
        "\n",
        "processed_text = [word for word in nopuch.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "word_freq = {}\n",
        "for word in processed_text:\n",
        "    if word not in word_freq:\n",
        "        word_freq[word] = 1\n",
        "    else:\n",
        "        word_freq[word] = word_freq[word] + 1\n",
        "\n",
        "max_freq = max(word_freq.values())\n",
        "for word in word_freq.keys():\n",
        "    word_freq[word] = (word_freq[word]/max_freq)\n",
        "\n",
        "sent_list = nltk.sent_tokenize(text)\n",
        "sent_score = {}\n",
        "for sent in sent_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_freq.keys():\n",
        "            if sent not in sent_score.keys():\n",
        "                sent_score[sent] = word_freq[word]\n",
        "            else:\n",
        "                sent_score[sent] = sent_score[sent] + word_freq[word]\n",
        "\n",
        "summary_sents = nlargest(length, sent_score, key=sent_score.get)\n",
        "summary = \" \".join(summary_sents)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "JTYF95dEZq66"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}